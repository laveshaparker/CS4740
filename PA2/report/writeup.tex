\documentclass{article}

\usepackage{amsmath,amsfonts,amsthm}
\usepackage{graphicx}
\usepackage[margin=1.3in]{geometry}

\setlength\parindent{0pt} 
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

\title{	
\normalfont \normalsize 
\textsc{Cornell University, INFO/CS 4740: Introduction to Natural Language Processing, Spring 2015} \\
\horrule{0.5pt} \\[0.4cm]
\huge PA 2: Question Answering \\ 
\horrule{2pt} \\[0.5cm]
}
\author{Sofonias Assefa (saa237), La Vesha Parker (ldp47)}
\date{\normalsize\today}
\begin{document}

\maketitle

\section{Overview}
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{images/diagram.jpg}
    \caption{Overview of our Question Answering system}
\end{figure}
At the very root of our QA system, we follow the description of the baseline system in the project writeup. We have three separate overall stages in our system, separated into three separate python classes:
\begin{enumerate}
\item Question Processing: \texttt{question\_formatter.py}
\item Passage Retrieval: \texttt{passage\_retrieval.py}
\item Answer Formation: \texttt{answer\_formation.py}
\end{enumerate}

\section{Question Processing}
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{images/question.png}
    \caption{Overview of the \texttt{Question} class.}
\end{figure}

This class provides our representation of the question that has been asked. The work we do in this step can be separated into two parts:
\begin{enumerate}
\item Basic question text processing
\item Making inferences about the nature of the question
\end{enumerate}

\subsection{Basic Question Text Processing}
We pull the text of the question from \texttt{questions.txt} and store its unique identifier, part of speech tags, and tokenized (both case-sensitive and case-insensitive) versions of the original text. All of this is done in the constructor of the \texttt{Question} class using the nltk library.

\subsection{Infer the Nature of the Question}
The work done in inferring the nature of the question is performed in the \texttt{Question} class, and the result is stored in the \texttt{descriptor} property of that class. The \texttt{descriptor} property on a \texttt{Question} instance stores the type of entity that we are looking for, along with some tokens in the tokenized question string that we believe are the most relevant.\\

We infer the nature of the question and its most relevant tokens by POS based pattern matching system. For example, upon inspecting the question files, we found that when the word \textit{"who"} appears in a question, we are most likely looking for a person or group in the answer. We found similar patterns for the words/phrases \textit{"where"},  \textit{"when"},  \textit{"how many"},  \textit{"how much"}, and  \textit{"what"}.\\

Given the appearance of those terms, we define the type of entity that is required in the answer (see the \texttt{REQUIREDENTITIES} map in the \texttt{Question} class).\\

Once we know the required type of entity, we attempt to infer relevant and descriptive tokens in the question for its required entity. For example, if a question begins with \textit{"who"}, the first noun phrase in the question is the best consistent clue as to answering the question. The list of possible inferences is in the \texttt{get\textless Entity\textgreater Descriptor} functions in the \texttt{Question} class by pattern matching on the part-of-speech tags of the question.\\

\section{Passage Retrieval}
The overall interface of this step is as follows: we receive a question (of type \texttt{Question}, available in \texttt{question\_formatter.py}) and output an instance of the \texttt{PassageRetrieval} class, which stores the most-similar passages for the top 10 documents in the \texttt{passages\_top\_10\_docs} property.
The steps taken during this step can be simplified as follows:
\begin{enumerate}
\item \textbf{Documents:} Process all the documents for a given question. In doing so, extract relevant text, identifiers, split into passages, etc.
\item \textbf{Term Frequency-Inverse Document Frequency:} Calculate the similarity between term frequency-inverse document frequency vectors between all passages and the question.
\item \textbf{Ranked Passages: }Return the top passage (sorted by tfidf) for each of the top 10 ranked documents.
\end{enumerate}

Examining each of these more closely we have:\\
\subsection{Documents}
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{images/document.png}
    \caption{Overview of the \texttt{Document} class.}
\end{figure}
\textit{Note: Processing the documents proved not to be as simple as we initially planned. The vast majority of the documents follow a similar markup structure, but a handful of them differed from the overall general structure. In those cases, we ignore the documents (don't include them in any of our subsequent system steps), as we found that those documents tend to be ranked low anyway.}\\

We begin processing the documents by pulling all of the text from the appropriate \texttt{top\_docs.$x$} file provided (with respect to dev or test) and separating it into entire documents, \texttt{qids} and all. Then, given the text of a document, we extract:
\begin{enumerate}
\item The \textit{rank} of the document.
\item The \textit{docno} (unique document identifier).
\item The relevancy \textit{score} of the document, as determined by the generator of the top\_docs documents.
\item The body (\textit{text}) of the document.
\item For the purpose of later calculations, we tokenize the document into sentences (\textit{sentences} and \textit{sentences\_w\_case}), and then those sentences into an array of tokens, with punctuation removed
\end{enumerate}

For the last extraction step, we remove punctuation because in a Question Answer system, the structure of the question is often different than the structure of the answer, so it would provide us little additional information. Also, we later use a tf-idf similarity measure so the punctuation may even hurt those calculations as certain punctuation doesn't occur frequently enough to be considered a stop word and would therefore have some effect on those final calculations.\\

Instead of defining a passage as a 10-gram, as is described in the baseline system, we define a passage as a sentence. Our justification for this is that the answer to a question is more likely to be contained in a sentence, as opposed to a sequence of 10 words, which might be spread over multiple sentences, including sentence fragments. This way, we were also able to simplify our Answer Formation step by knowing more of the general structure of the top-ranked passages that make it to that step.

\subsection{Tf-idf}
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{images/tfidf.png}
    \caption{Overview of the \texttt{TFIDF} class.}
\end{figure}

\textit{Note: In this implementation of tf-idf calculation, we do not use vectors the length of the number of unique terms in the document. Instead, for each text fragment (i.e. question text or document passage), we only include the terms involved in that particular text fragment to avoid having massive arrays with a ton of 0 term frequencies. So in calculating the cosine similarity of two tf-idf vectors, we just infer count 0 for any terms that don't appear in both of the tf-idf vectors.}\\

We decided to create our own tf-idf class instead of using a library so that we had more flexibility in terms of our overall interface. In this class, we first calculate the total term frequency of each unique term in the corpus (all relevant documents and the question). We do this using Python's Counter class to simplify the number of loops we would normally have to use. After this, we calculate the tf-idf vector of the question. We calculate the tf-idf vector of the question first so that we only have to worry about storing that vector, and can then go on to compute the tf-idf vectors of each passage and immediately find the similarity score with the question, so that we can avoid having to store the tf-idf vectors of every passage as an intermediate step to eventually finding the cosine similarity. Doing this sped up our QA system.

\subsection{Top Ten Passages}
As we calculate tf-idf similarity scores between each passage in each document and the question, we keep track of only the top passage and its similarity score for each document for later calculations of what passage we should send to the Answer Formation step in our system. This process outputs the top passage from each document that outputs the highest tfidf score with the question.

\section{Answer Formation}

The final step of our QA system uses the information inferred from the original question (from \texttt{question\_formatter.py}), and the top ten retrieved passages (from \texttt{passage\_retrieval.py}), to create ten distinct responses. This process three steps:

\begin{enumerate}
\item Locate all named entities and noun phrases in a given passage
\item Rank and aggregate the named entities and noun phrases according to relevance to the original question
\item Remove duplicate entries and return the top ten responses
\end{enumerate}

\subsection{Named Entitity and Noun Phrase Recognition}
Our answer formation process has two inputs: a question object (containing the nature of the question, and its most relevant tokens), and a ranked list of the top ten passages from the relevant documents. Here, we use the NLTK Named Entity Recognition library and Part of Speech library to locate all relevant named entities and all other noun phrases in each passage. If a passage does not contain a named entity, we make the assumption that this passage contains little vital information, and extract nothing is extracted from the passage. In many cases a passage that contains little valueable information may receive a high rank simply based on a large overlap of tokens with the question.

The selection of named entities is depenedent on the nature of the question; eg: if the question is asking for a location, all named entities that are not locations or geo-political entities are removed. Note that no pruning of the general noun phrases takes place at this step. This step outputs up to 20 lists of phrases: 10 containing the pruned named entities for each passage, and 10 containing the remaining noun phrases for each passage.

\subsection{Potential Response Ranking}
The contents of the 20 lists are then ranked and aggregated. Using the extracted relevant tokens from our question, the named entities within each passage are ranked based on distance (number of tokens) between the location of the relevant tokens in the passage and the location of named entitiy in the passage. This sorting process is repeated for the remaining extracted noun phrases. Each list of nouns is then pruned by the following assumption: a passage with more named entities is likely to contain a correct answer, so we reduce the number of extracted noun phrases to from a given passage to, at most, the number of extracted name entities from the same passage. 

The aggregation phase preserves the ranking of passages. We make the assumption that the rank of a passage, derived from the rank of its parent document in \texttt{passage\_retrieval.py}, quanitifies its likelihood of containing a correct answer. As a result, our aggregated list of potential responses has the following structure:

$$
[<Passage 1 Named Entities>, <Passage 1 Noun Phrases>, 
$$
$$
<Passage 2 Named Entities>, <Passage 2 Noun Phrases>, ... , 
$$
$$
<Passage 10 Named Entities>, <Passage 10 Noun Phrases>]
$$

\subsection{Duplicate Response Removal and Answer Finalization}
The ranked and aggregated list of potential responses is trimmed in two steps:
\begin{enumerate}
\item Remove all duplicate entries. If the same noun phrase or entity occures multiple times in the list, remove all but the highest ranked occurences
\item Remove all but the top ten ranked responses
\end{enumerate}

\section{Performance Analysis}
Using the provided \texttt{evaluation.py} script, our final QA system achieves a score of 0.1525. We attempted different techniques in our passage retrieval and answer formation steps that led to increased our score and led to the final system described above. The following heuristics gave us noticable improvements in score:
\begin{itemize}
\item Defining a relevant passage as an entire sentence vs. a 10-gram
\item Ranking named entities and noun phrases by their token distance from the relevant tokens extracted from the question (this allowed us to capture the fact that terms listed in close proximity to the relevant tokens are more likely to be a correct answer)
\item Our assumption that passages that contain no named entities do not contain the correct answer
\item Pruning each list of extracted noun phrases to, at most, the size of its corresponding named entity list
\end{itemize}

\section{Dependencies}
Our system heavily uses the NLTK library at all three levels of our QA process. In particular we utilize the following components:
	\begin{itemize}
	\item POS-Tagger
	\item Sentence Tokenizer
	\item Word Tokenizer
	\item Named Entity Chunker
	\item Corpus of Stop Words
	\end{itemize}
\end{document}
